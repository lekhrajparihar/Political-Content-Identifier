{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reddit_extract_keywords.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"18orQAM8Hu36d58z8IMph-qvKuL__mTaW","authorship_tag":"ABX9TyMLWgruv+5moLhyw2/ONoud"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"Oe_DJ9dTuBp0"},"source":["!pip install rake-nltk\n","!pip install git+https://github.com/boudinfl/pke.git\n","!pip install swifter\n","!pip install fasttext --upgrade \n","!pip3 install summa\n","!pip3 install pandarallel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igQHDyeMyaVs"},"source":["from pandarallel import pandarallel\n","import re\n","import string\n","import sys\n","import html\n","import pandas as pd\n","import json\n","import csv\n","import glob\n","import os.path\n","import time\n","import nltk\n","import logging as lg\n","from datetime import datetime as dt, timedelta\n","from gensim.summarization import keywords\n","from random import *\n","import numpy as np\n","from gensim.utils import simple_preprocess"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GruVr-m-yVCY"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#drive.flush_and_unmount()\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","pandarallel.initialize()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QN9J_kzvzAnv"},"source":["punc = '''!()[]|{};\":\\<>/@#$%^&*_~'''\n","\n","#Preprocess text, punctuations etc.\n","relp = \" \"*len(punc)\n","def preprocess(s):\n","  result = re.sub(r\"â€™\", \"'\", str(s))\n","  result = re.sub(r\"\\n\", ' ', str(result))\n","  result = re.sub(r\"r/\", ' ', str(result))\n","  result = result.encode(\"ascii\", \"ignore\").decode()\n","  result = re.sub(r\"http\\S+\", ' ', str(result), flags=re.MULTILINE)\n","  result = html.unescape(result)\n","  result = result.translate(str.maketrans(punc,relp))\n","  result = re.sub(' +', ' ',str(result))\n","  result = result.strip()\n","  return result;"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hp3aMNvRyyR8"},"source":["#Load dataset\n","df = pd.read_csv(\"/content/drive/My Drive/datasets/reddit/raw_dataset.csv\",engine=\"python\")\n","df[\"subreddit\"] = df[\"subreddit\"].str.lower()\n","df[\"text\"] = df[\"text\"].apply(lambda x: '\\n'.join(preprocess(x.splitlines())))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NRsLb5rqoKBm"},"source":["print(df.info)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94n1JR6kyDl7"},"source":["pol_arr = [\"politics\",\"politicalhumor\",\"politicalcompassMemes\",\"conservative\",\n","            \"therightcantmeme\",\"neoliberal\",\"democrats\",\"politicaldiscussion\",\n","            \"republicans\", \"libertarian\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGM1NkmWKcwm"},"source":["#Extract keywords using textrank implementation in gensim\n","def keywords_tr(x):\n","  kws = keywords(x,ratio=1,lemmatize=True,scores=True)[:10]\n","  return json.dumps(dict(kws))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o89iHO4M8eZZ"},"source":["#TextRank is ridiculously slow, and I can't find any gpu implementations of it. \n","#So processing in chunks using multiprocessing.\n","def split_dataframe(df, chunk_size = 5000): \n","    chunks = list()\n","    num_chunks = len(df) // chunk_size + 1\n","    for i in range(num_chunks):\n","        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n","    return chunks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wds4OPDVre6z"},"source":["#Split and apply\n","i = 0\n","for df_chunk in split_dataframe(df,chunk_size=1000):\n","  print(i)\n","  df_chunk[\"text\"]=df_chunk[\"text\"].parallel_apply(lambda x: keywords_tr(x))\n","  df_chunk.to_csv(f\"/content/drive/My Drive/datasets/reddit/keywords/{i}.csv\",index=False)\n","  i+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONpXOpNARbrF"},"source":["#Merge\n","path = r'/content/drive/My Drive/datasets/reddit/keywords/' \n","all_files = glob.glob(path + \"/*.csv\")\n","\n","li = []\n","\n","for filename in all_files:\n","    df = pd.read_csv(filename, index_col=None, header=0)\n","    li.append(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yYN3u9xrYngv"},"source":["#Final Dataframe    \n","dff = pd.concat(li, axis=0, ignore_index=True)\n","dff[\"keywords\"] = dff[\"text\"].apply(lambda x: list(json.loads(x).keys()))\n","dff[\"text_rank_scores\"] = dff[\"text\"].apply(lambda x: list(json.loads(x).values()))\n","dff[\"label\"]=dff[\"subreddit\"].apply(lambda x:x in pol_arr)\n","dff.drop(columns=[\"text\"],inplace=True)\n","dff.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUGYk3Nucgex"},"source":["dff.to_csv(\"/content/drive/My Drive/datasets/reddit/extracted_keywords_freq.csv\",index=False)"],"execution_count":null,"outputs":[]}]}